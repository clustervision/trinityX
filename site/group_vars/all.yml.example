---
# This file contains variables that are global to all playbooks
# in this repository.

# Any alerts, events and notifications will be sent to this email address:

administrator_email: 'root@localhost'


# -----------------------------------------------------------------------
# Trinity version number

trix_version: '14'

# What stream to follow. Stable is the default and recommended.
trix_stream: 'stable'

# -----------------------------------------------------------------------
# Project ID or string that'll show up in the default
# prompt on the controllers.

project_id: '000000'


# -----------------------------------------------------------------------
# Do we want HA?
# Set to 'False' to disable HA, set to 'True' to enable it.

ha: false


# -----------------------------------------------------------------------
# Should we use content of the installation CD/USB-stick
# to install TrinityX

local_install: false


# -----------------------------------------------------------------------
# A domain name to be assigned to the controller(s) and nodes
# on the internal network.
# Luna's provisioning network name will be set to the same value.

trix_domain: cluster


# -----------------------------------------------------------------------
# Default hostname and IP for the controller
# In an HA pair, those are the hostname and IP for the first controller.
# Those variables are required, with or without HA.

trix_ctrl1_ip: 10.141.255.254
trix_ctrl1_bmcip: 10.148.255.254
trix_ctrl1_heartbeat_ip: 10.146.255.254
trix_ctrl1_hostname: controller1

# In a non-HA setup, all of the following variables will be ignored:
# - the variables for CTRL will be set to the same as CTRL1;
# - the variables for CTRL2 will be ignored.

# Hostname and IP of the second controller

trix_ctrl2_ip: 10.141.255.253
trix_ctrl2_bmcip: 10.148.255.253
trix_ctrl2_heartbeat_ip: 10.146.255.253
trix_ctrl2_hostname: controller2

# Floating hostname and IP

trix_ctrl_ip: 10.141.255.252
trix_ctrl_hostname: controller

# Internal network definitions and the DHCP range used to to PXE boot
# the compute nodes.

trix_cluster_net: 10.141.0.0
trix_cluster_netprefix: 16
trix_cluster_dhcp_start: 10.141.128.0
trix_cluster_dhcp_end: 10.141.140.0


# -----------------------------------------------------------------------
# optional hostname/fqdn to be used for services that are outward facing,
# like open-ondemand. This hostname is in most scenarios the fqdn of the host,
# but this can be overridden if external resolution/name differs.
# The fqdn of the host will be used when Left empty or open.

trix_external_fqdn: ''


# -----------------------------------------------------------------------
# DNS forwarders.
trix_dns_forwarders: 
  - "8.8.8.8"
  - "8.8.4.4"


# -----------------------------------------------------------------------
# Default search domains to be added to /etc/resolv.conf

resolv_search_domains: '{{ trix_domain }} ipmi'


# -----------------------------------------------------------------------
# Path to which the standard TrinityX files will be installed.
# If not set, it will default to /trinity

trix_root: '/trinity'

# The TrinityX root contains multiple subdirectories for different uses:
# - trix_images For the compute node images
# - trix_shared for everything shared by the controllers to the nodes
# - trix_local for configuration files specific to each machine
# - trix_home for the user home directories
#
# Since trix_local will also house the monitoring data, it is recommended to
# put this on a seperate partition for large installations or extensive
# monitoring setups.
# The partitioning on the controller must be set up before starting the installation.
#
# By default those exist under trix_root. There are some cases where this is not
# desirable; the following variables allow you to override the default paths.
# WARNING: use with caution! It's not well tested and there may be some issues
# here and there...

trix_images: '{{ trix_root }}/images'
trix_shared: '{{ trix_root }}/shared'
trix_local: '{{ trix_root }}/local'
trix_luna: '{{ trix_local }}/luna'
trix_etc: '{{ trix_local }}/etc'
trix_sbin: '{{ trix_local }}/sbin'
trix_ood: '{{ trix_local }}/ondemand'
trix_ssl: '{{ trix_etc }}/ssl'
trix_home: '{{ trix_root }}/home'
trix_repos: '{{ trix_root }}/repos'
trix_ohpc: '{{ trix_root }}/ohpc'
trix_licenses: '{{ trix_shared }}/licenses'
trix_docs: '{{ trix_shared }}/docs'
trix_examples: '{{ trix_shared }}/examples'
trix_modulefiles: '{{ trix_shared }}/modulefiles'


# -----------------------------------------------------------------------
# Luna credentials. These should be changed.

luna_username: luna
luna_password: luna


# luna's default communication protocol
# options are: http and https
# by default the system is using the certificates provides and signed by self-signed CV root ca
# in this case we do not verify the certificate

luna_protocol: https
luna_verify_certificate: false


# -----------------------------------------------------------------------
# Default firewalld configuration
# Only public tcp/udp ports are allowed on the public interfaces
# whereas everything is allowed on the trusted interfaces

firewalld_public_interfaces: [ens3]
firewalld_trusted_interfaces: [ens6]
firewalld_public_tcp_ports: [22, 443]
firewalld_public_udp_ports: []

# *Note* that depending on selection of roles, the following ports will be open as well:
# Grafana:       3000/tcp
# Uchiwa/Sensu:  3001/tcp
# Open-ondemand: 8080/tcp


# -----------------------------------------------------------------------
# Admin group is used to give privileges to the members of this group.
# It's used for logins, ood and others.

admin_group: admins


# -----------------------------------------------------------------------
# Path in which the generated certificates for the cluster will be installed.
# ssl_cert_group refers to the UNIX group that has read access to the certs.

ssl_cert_path: '{{ trix_local }}/etc/ssl'
ssl_ca_cert: '{{ ssl_cert_path }}/cluster-ca.crt'
ssl_cert_group: ssl


# -----------------------------------------------------------------------
# OpenHPC repositories. commonly used for openhpc and schedulers

# --- for EL8:
el8_openhpc_repositories:
  - name: base
    repo: http://repos.openhpc.community/OpenHPC/2/EL_8/
    gpgcheck: false
  - name: updates
    repo: http://repos.openhpc.community/OpenHPC/2/update.2.6.1/EL_8/
    gpgcheck: false

# --- for EL9:
el9_openhpc_repositories:
  - name: base
    repo: https://updates.clustervision.com/trinityx/external/openhpc/redhat/9
    gpgcheck: false


# -----------------------------------------------------------------------
# set the default target for images. 
# available: graphical and multi-user

image_default_target: 'multi-user'


# =======================================================================
# The following variables give the ability to enable or disable
# certain features in TrinityX:

# -----------------------------------------------------------------------
# Whether or not to enable SELinux throughout the cluster

enable_selinux: true


# -----------------------------------------------------------------------
# Whether or not to configure the corosync heartbeat link (ring 1)

enable_heartbeat_link: true


# -----------------------------------------------------------------------
# Install docker daemon, registry and utilities on the cluster

enable_docker: false


# -----------------------------------------------------------------------
# List of additional environment modules to install

additional_env_modules: []


# -----------------------------------------------------------------------
# Install userspace packages from OpenHPC project

enable_openhpc: true


# -----------------------------------------------------------------------
# Default workload manager. Currently supported are:
# - slurm
# - pbspro

workload_manager: 'slurm'

# -----------------------------------------------------------------------
# Whether or not to enable Slurm PAM module
# If enabled, sssd's ldap filters will be disabled on the compute nodes

enable_slurm_pam: true


# -----------------------------------------------------------------------
# Enable VNC to nodes on Open OnDemand

enable_ood_vnc: true


# -----------------------------------------------------------------------
# whether we need to configure our ldap, sssd and other things (true)
# or we use an external manually configured authentication mechanism (false)

enable_authentication: true

