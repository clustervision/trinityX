---
# This file contains variables that are global to all playbooks
# in this repository.


# Any alerts, events and notifications will be sent to this email address:

administrator_email: 'root@localhost'


# -----------------------------------------------------------------------
# Trinity version number

trix_version: '15.2'

# What stream to follow. Stable is the default and recommended.
trix_stream: 'stable'

# -----------------------------------------------------------------------
# Project ID or string that'll show up in the default
# prompt on the controllers.

project_id: '000000'


#########################################################################
#                           Cloud Details                               #
#########################################################################

# Which Cloud Providers need to be used for this installation.
# Currently only azure_cloud is supported.
azure_cloud: false
aws_cloud: false
gcp_cloud: false  # gcp not supported yet. keep at false

# Detailed configuration can be found in group_vars/<cloud>.yml
# Please consider configuring these yml files, before proceeding

# -----------------------------------------------------------------------
# optional overrides. If not defined or left empty, ansible attempts to figure
# out the IP address on the outside (AS) of your network. This variable is
# used by cloud VPN.

# cloud_local_external_as_ip:


#########################################################################
#                         High Availability                             #
#########################################################################

# Do we want HA?
# Set to 'False' to disable HA, set to 'True' to enable it.

ha: false

# -----------------------------------------------------------------------
# whether we want to use ipmi for H/A fencing

enable_ipmilan_fencing: true

# -----------------------------------------------------------------------
# IPMI H/A fencing credentials. Used in conjunction with HA
# Being ignored if setting enable_ipmilan_fencing is set to false

fence_ipmilan_login: 'user'
fence_ipmilan_passwd: 'password'

# note: the $ character is not support in the ipmilan login or passwd

# -----------------------------------------------------------------------
# Whether or not to configure the corosync heartbeat link (ring 1)
# Requires correct configured trix_ctrlX_heartbeat_ip.

enable_heartbeat_link: true


#########################################################################
#                         Network configuration                         #
#########################################################################

# A domain name to be assigned to the controller(s) and nodes
# on the internal network.
# Luna's provisioning network name will be set to the same value.

trix_domain: cluster


# -----------------------------------------------------------------------
# Internal network definitions and the DHCP range used to to PXE boot
# the compute nodes. Also Infiniband IPoIB and bmc/ipmi networks.

trix_cluster_net: 10.141.0.0
trix_cluster_netprefix: 16
trix_cluster_dhcp_start: 10.141.128.0
trix_cluster_dhcp_end: 10.141.140.0

trix_infiniband_net: 10.149.0.0
trix_infiniband_netprefix: 16

trix_bmc_net: 10.148.0.0
trix_bmc_netprefix: 16


# -----------------------------------------------------------------------
# Default hostname and IP for the controller
# In an H/A pair, those are the hostname and IP for the first controller.
# Those variables are required, with or without HA.

trix_ctrl1_ip: 10.141.255.254
trix_ctrl1_bmcip: 10.148.255.254
trix_ctrl1_heartbeat_ip: 10.146.255.254
trix_ctrl1_hostname: controller1

# In a non-H/A setup, all of the following variables will be ignored:
# - the variables for CTRL will be set to the same as CTRL1;
# - the variables for CTRL2 will be ignored.

# Hostname and IP of the second controller

trix_ctrl2_ip: 10.141.255.253
trix_ctrl2_bmcip: 10.148.255.253
trix_ctrl2_heartbeat_ip: 10.146.255.253
trix_ctrl2_hostname: controller2

# Floating hostname and IP

trix_ctrl_ip: 10.141.255.252
trix_ctrl_hostname: controller


# -----------------------------------------------------------------------
# Optional overrides when defaults do not fullfil what's expected
# When left undefined or empty, ansible will use the IP/network address
# assigned to the interface where the default route is defined for.
# trix_ctrl_external_network is used for e.g. cloud VPN.

# trix_ctrl_external_ip:
# trix_ctrl_external_network: # w.x.y.z/nn


# -----------------------------------------------------------------------
# HA: Configure a floating ip address on the external interfaces (read:
# outbound interfaces) as well. The trix_ctrl_external_nic can be set
# but is optional. TrinityX tries to figure this out based on the default
# route, which most likely uses the preferred external nic. If netprefix is
# not given, it'll be tried by looking at trix_ctrl_external_network.

# trix_ctrl_external_nic:         # e.g. ens192
# trix_ctrl_external_floating_ip: # w.x.y.z


# -----------------------------------------------------------------------
# quasi optional hostname/fqdn to be used for services that are outward facing,
# like open-ondemand. This hostname is in most scenarios the fqdn of the host,
# but this can be overridden if external resolution/name differs.

trix_external_fqdn: ''


# -----------------------------------------------------------------------
# optional hostname/fqdn only needed when building a login image
# based on the login-default.yml playbook.

trix_login_fqdn: ''


# -----------------------------------------------------------------------
# DNS forwarders. These should normally point to your organization's DNS servers.

trix_dns_forwarders: 
  - "8.8.8.8"
  - "8.8.4.4"


# -----------------------------------------------------------------------
# Default search domains to be added to /etc/resolv.conf
# __CLOUD__ is a placeholder for the cloud domains if applicable.
# It will be searched/replaced for the actual configured domains.
# This placeholder will be removed and can be ignored safely.

resolv_search_domains: '{{ trix_domain }} __CLOUD__ ib ipmi'


#########################################################################
#                          Paths configuration                          #
#########################################################################
#
# Path to which the standard TrinityX files will be installed.
# If not set, it will default to /trinity

trix_root: '/trinity'

# The TrinityX root contains multiple subdirectories for different uses:
# - trix_images For the compute node images
# - trix_shared for everything shared by the controllers to the nodes
# - trix_local for configuration files specific to each machine. This one
#              is not listed and hard configured to /trinity/local.
# - trix_home for the user home directories
#
# The partitioning on the controller must be set up before starting the installation.
#
# By default those exist under trix_root. There are some cases where this is not
# desirable; the following variables allow you to override the default paths.
# WARNING: use with caution! It's not well tested and there may be some issues
# here and there...

trix_images: '{{ trix_root }}/images'
trix_shared: '{{ trix_root }}/shared'
trix_ha: '{{ trix_root }}/ha'
trix_home: '{{ trix_root }}/home'
trix_repos: '{{ trix_root }}/repos'
trix_ohpc: '{{ trix_shared }}/ohpc'
trix_easybuild: '{{ trix_shared }}/easybuild'
trix_licenses: '{{ trix_shared }}/licenses'
trix_docs: '{{ trix_shared }}/docs'
trix_examples: '{{ trix_shared }}/examples'
trix_modulefiles: '{{ trix_shared }}/modulefiles'


#########################################################################
#                      Shared Disk configuration                        #
#########################################################################

# Backend type and block device to use for the specified backend.
# Works in conjunction with H/A. Combinations are supported.
# 'type' can be: drbd, nfs, iscsi or direct (e.g. multipath).
# 'fstype' can be: ext4, xfs or any regular filesystem
#                  but also lvm or zfs. In this case
#                  partitions have to be defined.
# When 'type' is set to 'manual', this indicates that this is being
# taken care off outside TrinityX

# --- examples:
#
# shared_fs_disks: 
#   - name: 'ZFS1'
#     type: 'drbd'
#     disk: '/dev/sda4'
#     device: '/dev/drbd4'
#     fstype: 'zfs'
#     partitions:
#     - name: '/trinity/other'
#       fsoptions: '-o xattr=sa'
#
#   - name: 'LVM1'
#     type: 'drbd'
#     disk: '/dev/sda3'
#     device: '/dev/drbd3'
#     fstype: 'lvm'
#     partitions:
#     - mount: '{{ trix_shared }}'
#       size: '20480M'
#       fstype: 'ext4'
#       fsoptions: '-b 4096'
#       xmount: true
#     - mount: '{{ trix_ohpc }}'
#       size: '10240M'
#       fstype: 'ext4'
#       fsoptions: '-b 4096'
#
#   - name: 'Home directories'
#     mount: '{{ trix_home }}'
#     type: 'drbd'
#     disk: '/dev/sda2'
#     device: '/dev/drbd2'
#     fstype: 'ext4'
#     fsoptions: '-b 4096 -O quota -E quotatype=usrquota:grpquota'
#     mountoptions: 'quota,usrquota,grpquota'
#     xmount: true
#
#   - name: 'LVM2'
#     type: 'iscsi'
#     fstype: 'lvm'
#     portal: '192.168.0.1'
#     target: 'iqn.2003-01.org.linux-iscsi.iscsi-host.x8664:sn.693ca380655e'
#     partitions:
#     - mount: '/trinity/ext1'
#       size: '8192M'
#       fstype: 'ext4'
#       fsoptions: '-b 4096'
#
#   - name: 'LVM3'
#     type: 'direct'
#     device: /dev/sdz
#     fstype: 'lvm'
#     partitions:
#     - mount: '/trinity/ext2'
#       size: '16384M'
#       fstype: 'ext4'
#       fsoptions: '-b 4096'
#
#   - name: '{{ trix_home }}'
#     mount: '{{ trix_home }}'
#     type: 'nfs'
#     remote: '192.168.0.1:/homes'
#     mountoptions: 'nfsvers=4,rw,retrans=4,_netdev'
#
#   - name: '{{ trix_home }}'
#     type: 'manual'
#

# As an absolute bare minimum trix_ha requires a shared disk.
# For trix_home and trix_shared it comes extremely highly
# recommended to have a shared disk as well. Below is what's
# considered a standard shared disk configuration. Note that /dev/vda
# needs to be configured to be a real disk which is 100% equal
# on each controller when using drbd.

# !! formerly known parameter 'options' has been renamed into 'fsoptions'.
# !! mountoptions was introduced to allow pcs to mount a disk according to these.

shared_fs_disks:
  - name: 'trinityx'
    type: 'drbd'
    disk: '/dev/vda'
    device: '/dev/drbd0'
    fstype: 'zfs'
    partitions:
    - mount: '{{ trix_home }}'
      fsoptions: '-o xattr=sa'
      xmount: true
    - mount: '{{ trix_shared }}'
      fsoptions: '-o xattr=sa'
      xmount: true
    - mount: '{{ trix_ha }}'
      fsoptions: '-o xattr=sa'
    

# shared_fs_prefix is used in combination with xmount: true (cross mount);
# you want all controllers to be able to mount the shared disks, using NFS,
# through the shared ip.
# when xmount is false or shared_fs_prefix is empty, this functionality
# will be disabled and shared_fs_disks will be mounted like TrinityX 14.x,
# only on the active controller.
# Note: this settings only affects drbd, iscsi and direct mounted disks.

shared_fs_prefix: '{{ trix_root }}/mounts'


#########################################################################
#                     Luna specific configuration                       #
#########################################################################

# Luna credentials. These !! SHOULD !! be changed.

luna_username: luna
luna_password: luna


# luna's default communication protocol
# options are: http and https
# by default the system is using the certificates provides and signed by self-signed CV root ca
# in this case we do not verify the certificate

luna_protocol: https
luna_verify_certificate: false


#########################################################################
#                        Firewall configuration                         #
#########################################################################

# Default firewalld configuration
# Only public tcp/udp ports are allowed on the public interfaces
# whereas everything is allowed on the trusted interfaces

firewalld_public_interfaces: [ens3]
firewalld_trusted_interfaces: [ens6]
firewalld_public_tcp_ports: [22, 443]
firewalld_public_udp_ports: []

# *Note* that depending on selection of roles, the following ports will be open as well:
# Grafana:       3000/tcp
# Uchiwa/Sensu:  3001/tcp
# Open-ondemand: 8080/tcp


#########################################################################
#                        Groups and certificates                        #
#########################################################################

# Admin group is used to give privileges to the members of this group.
# It's used for logins, ood and others.

admin_group: admins


# -----------------------------------------------------------------------
# Path in which the generated certificates for the cluster will be installed.
# ssl_cert_group refers to the UNIX group that has read access to the certs.

ssl_cert_path: '{{ trix_local }}/etc/ssl'
ssl_ca_cert: '{{ ssl_cert_path }}/cluster-ca.crt'
ssl_ca_key: '{{ ssl_cert_path }}/cluster-ca.key'
ssl_cert_group: ssl


#########################################################################
#                           Open Ondemand                               #
#########################################################################

# Choose the auth provider for Open OnDemand
# - pam
# - dex  ( only openldap is configured for dex )
# Note that setting enable_authentication to false, implies 'pam' here.

ood_authentication_provider: 'dex'


# -----------------------------------------------------------------------
# Optional overrides for Open OnDemand

ood_enable_interactive_apps:
  - codeserver #(Third party)
  - jupyter #(Third party)
  - desktop


#########################################################################
#                           Miscellaneous                               #
#########################################################################

# OpenHPC repositories. commonly used for openhpc and schedulers

# --- for EL8:
el8_openhpc_repositories:
  - name: base
    repo: http://repos.openhpc.community/OpenHPC/2/EL_8/
    gpgcheck: false
  - name: updates
    repo: http://repos.openhpc.community/OpenHPC/2/update.2.9.1/EL_8/
    gpgcheck: false

# --- for EL9:
el9_openhpc_repositories:
  - name: base
    repo: http://repos.openhpc.community/OpenHPC/3/EL_9/
    gpgcheck: false
  - name: updates
    repo: http://repos.openhpc.community/OpenHPC/3/update.3.3.1/EL_9/
    gpgcheck: false


# -----------------------------------------------------------------------
# set the default target for images. 
# available: graphical and multi-user

image_default_target: 'multi-user'


# =======================================================================
# The following variables give the ability to enable or disable
# certain features in TrinityX:


# -----------------------------------------------------------------------
# Whether or not to enable SELinux throughout the cluster

enable_selinux: true


# -----------------------------------------------------------------------
# List of additional environment modules to install

additional_env_modules: []


# -----------------------------------------------------------------------
# install Open Ondemand and its components

enable_ood: true


# -----------------------------------------------------------------------
# Install userspace packages from OpenHPC project

enable_openhpc: true


# -----------------------------------------------------------------------
# Default workload manager. Currently supported are:
# - slurm
# - pbspro
# - k3s   <-- unselected by default. For Kubernetes support, please uncomment
#             the k3s line in the below workload_manager.

workload_manager: 
  - 'slurm'
#  - 'k3s'

# -----------------------------------------------------------------------
# Whether or not to enable Slurm PAM module
# If enabled, sssd's ldap filters will be disabled on the compute nodes

enable_slurm_pam: true


# -----------------------------------------------------------------------
# Whether we install and prepare the easybuild environment

enable_easybuild: false
export_easybuild: 'rw'

# -----------------------------------------------------------------------
# whether we need to configure our ldap, sssd and other things (true)
# or we use an external manually configured authentication mechanism (false)

enable_authentication: true



# -----------------------------------------------------------------------
# DO NOT REMOVE: yml check: 150200
